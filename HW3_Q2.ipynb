{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjcwJzjCJ6OS"
      },
      "source": [
        "**Preparing libraries:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nErN6ijJv-c",
        "outputId": "e93c140b-6cf1-4a68-e697-4898a7bc36a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting fiftyone\n",
            "  Downloading fiftyone-0.23.2-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles (from fiftyone)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting argcomplete (from fiftyone)\n",
            "  Downloading argcomplete-3.2.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (4.11.2)\n",
            "Collecting boto3 (from fiftyone)\n",
            "  Downloading boto3-1.34.7-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from fiftyone) (5.3.2)\n",
            "Collecting dacite<1.8.0,>=1.6.0 (from fiftyone)\n",
            "  Downloading dacite-1.7.0-py3-none-any.whl (12 kB)\n",
            "Collecting Deprecated (from fiftyone)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting ftfy (from fiftyone)\n",
            "  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: humanize in /usr/local/lib/python3.10/dist-packages (from fiftyone) (4.7.0)\n",
            "Collecting hypercorn>=0.13.2 (from fiftyone)\n",
            "  Downloading hypercorn-0.15.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Jinja2>=3 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (3.1.2)\n",
            "Collecting kaleido!=0.2.1.post1 (from fiftyone)\n",
            "  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from fiftyone) (3.7.1)\n",
            "Collecting mongoengine==0.24.2 (from fiftyone)\n",
            "  Downloading mongoengine-0.24.2-py3-none-any.whl (108 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.9/108.9 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting motor>=2.5 (from fiftyone)\n",
            "  Downloading motor-3.3.2-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fiftyone) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fiftyone) (23.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from fiftyone) (1.5.3)\n",
            "Requirement already satisfied: Pillow>=6.2 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (9.4.0)\n",
            "Requirement already satisfied: plotly>=4.14 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (5.15.0)\n",
            "Collecting pprintpp (from fiftyone)\n",
            "  Downloading pprintpp-0.4.0-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from fiftyone) (5.9.5)\n",
            "Collecting pymongo>=3.12 (from fiftyone)\n",
            "  Downloading pymongo-4.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (677 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m677.1/677.1 kB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from fiftyone) (2023.3.post1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from fiftyone) (6.0.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fiftyone) (2023.6.3)\n",
            "Collecting retrying (from fiftyone)\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from fiftyone) (1.2.2)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from fiftyone) (0.19.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from fiftyone) (67.7.2)\n",
            "Collecting sseclient-py<2,>=1.7.2 (from fiftyone)\n",
            "  Downloading sseclient_py-1.8.0-py2.py3-none-any.whl (8.8 kB)\n",
            "Collecting sse-starlette<1,>=0.10.3 (from fiftyone)\n",
            "  Downloading sse_starlette-0.10.3-py3-none-any.whl (8.0 kB)\n",
            "Collecting starlette>=0.24.0 (from fiftyone)\n",
            "  Downloading starlette-0.34.0-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.3/70.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting strawberry-graphql==0.138.1 (from fiftyone)\n",
            "  Downloading strawberry_graphql-0.138.1-py3-none-any.whl (192 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.5/192.5 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fiftyone) (0.9.0)\n",
            "Collecting xmltodict (from fiftyone)\n",
            "  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
            "Collecting universal-analytics-python3<2,>=1.0.1 (from fiftyone)\n",
            "  Downloading universal_analytics_python3-1.1.1-py3-none-any.whl (10 kB)\n",
            "Collecting fiftyone-brain<0.15,>=0.14.2 (from fiftyone)\n",
            "  Downloading fiftyone_brain-0.14.2-py3-none-any.whl (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.5/88.5 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fiftyone-db<2.0,>=0.4 (from fiftyone)\n",
            "  Downloading fiftyone_db-1.1.tar.gz (7.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting voxel51-eta<0.13,>=0.12.1 (from fiftyone)\n",
            "  Downloading voxel51_eta-0.12.1-py2.py3-none-any.whl (570 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m570.1/570.1 kB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from fiftyone) (4.8.1.78)\n",
            "Collecting graphql-core<3.3.0,>=3.2.0 (from strawberry-graphql==0.138.1->fiftyone)\n",
            "  Downloading graphql_core-3.2.3-py3-none-any.whl (202 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.9/202.9 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from strawberry-graphql==0.138.1->fiftyone) (2.8.2)\n",
            "Requirement already satisfied: typing_extensions<5.0.0,>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from strawberry-graphql==0.138.1->fiftyone) (4.5.0)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from fiftyone-brain<0.15,>=0.14.2->fiftyone) (1.11.4)\n",
            "Collecting h11 (from hypercorn>=0.13.2->fiftyone)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h2>=3.1.0 (from hypercorn>=0.13.2->fiftyone)\n",
            "  Downloading h2-4.1.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting priority (from hypercorn>=0.13.2->fiftyone)\n",
            "  Downloading priority-2.0.0-py3-none-any.whl (8.9 kB)\n",
            "Collecting taskgroup (from hypercorn>=0.13.2->fiftyone)\n",
            "  Downloading taskgroup-0.0.0a4-py2.py3-none-any.whl (9.1 kB)\n",
            "Requirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from hypercorn>=0.13.2->fiftyone) (2.0.1)\n",
            "Collecting wsproto>=0.14.0 (from hypercorn>=0.13.2->fiftyone)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3->fiftyone) (2.1.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=4.14->fiftyone) (8.2.3)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo>=3.12->fiftyone)\n",
            "  Downloading dnspython-2.4.2-py3-none-any.whl (300 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.4/300.4 kB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette>=0.24.0->fiftyone) (3.7.1)\n",
            "Collecting httpx>=0.10.0 (from universal-analytics-python3<2,>=1.0.1->fiftyone)\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill (from voxel51-eta<0.13,>=0.12.1->fiftyone)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from voxel51-eta<0.13,>=0.12.1->fiftyone) (0.18.3)\n",
            "Requirement already satisfied: glob2 in /usr/local/lib/python3.10/dist-packages (from voxel51-eta<0.13,>=0.12.1->fiftyone) (0.7)\n",
            "Collecting jsonlines (from voxel51-eta<0.13,>=0.12.1->fiftyone)\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Collecting py7zr (from voxel51-eta<0.13,>=0.12.1->fiftyone)\n",
            "  Downloading py7zr-0.20.8-py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rarfile (from voxel51-eta<0.13,>=0.12.1->fiftyone)\n",
            "  Downloading rarfile-4.1-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from voxel51-eta<0.13,>=0.12.1->fiftyone) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from voxel51-eta<0.13,>=0.12.1->fiftyone) (1.16.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from voxel51-eta<0.13,>=0.12.1->fiftyone) (2.4.0)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from voxel51-eta<0.13,>=0.12.1->fiftyone) (5.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from voxel51-eta<0.13,>=0.12.1->fiftyone) (2.0.7)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->fiftyone) (2.5)\n",
            "Collecting botocore<1.35.0,>=1.34.7 (from boto3->fiftyone)\n",
            "  Downloading botocore-1.34.7-py3-none-any.whl (11.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->fiftyone)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->fiftyone)\n",
            "  Downloading s3transfer-0.10.0-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from Deprecated->fiftyone) (1.14.1)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->fiftyone) (0.2.12)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fiftyone) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fiftyone) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fiftyone) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fiftyone) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fiftyone) (3.1.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->fiftyone) (3.2.1)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->fiftyone) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->fiftyone) (2023.12.9)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->fiftyone) (1.5.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fiftyone) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fiftyone) (3.2.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette>=0.24.0->fiftyone) (3.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette>=0.24.0->fiftyone) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette>=0.24.0->fiftyone) (1.2.0)\n",
            "Collecting hyperframe<7,>=6.0 (from h2>=3.1.0->hypercorn>=0.13.2->fiftyone)\n",
            "  Downloading hyperframe-6.0.1-py3-none-any.whl (12 kB)\n",
            "Collecting hpack<5,>=4.0 (from h2>=3.1.0->hypercorn>=0.13.2->fiftyone)\n",
            "  Downloading hpack-4.0.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.10.0->universal-analytics-python3<2,>=1.0.1->fiftyone) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx>=0.10.0->universal-analytics-python3<2,>=1.0.1->fiftyone)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines->voxel51-eta<0.13,>=0.12.1->fiftyone) (23.1.0)\n",
            "Collecting texttable (from py7zr->voxel51-eta<0.13,>=0.12.1->fiftyone)\n",
            "  Downloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting pycryptodomex>=3.16.0 (from py7zr->voxel51-eta<0.13,>=0.12.1->fiftyone)\n",
            "  Downloading pycryptodomex-3.19.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyzstd>=0.15.9 (from py7zr->voxel51-eta<0.13,>=0.12.1->fiftyone)\n",
            "  Downloading pyzstd-0.15.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (412 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.3/412.3 kB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyppmd<1.2.0,>=1.1.0 (from py7zr->voxel51-eta<0.13,>=0.12.1->fiftyone)\n",
            "  Downloading pyppmd-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.9/138.9 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pybcj<1.1.0,>=1.0.0 (from py7zr->voxel51-eta<0.13,>=0.12.1->fiftyone)\n",
            "  Downloading pybcj-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multivolumefile>=0.2.3 (from py7zr->voxel51-eta<0.13,>=0.12.1->fiftyone)\n",
            "  Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n",
            "Collecting inflate64<1.1.0,>=1.0.0 (from py7zr->voxel51-eta<0.13,>=0.12.1->fiftyone)\n",
            "  Downloading inflate64-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting brotli>=1.1.0 (from py7zr->voxel51-eta<0.13,>=0.12.1->fiftyone)\n",
            "  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m108.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->voxel51-eta<0.13,>=0.12.1->fiftyone) (3.3.2)\n",
            "Building wheels for collected packages: fiftyone-db\n",
            "  Building wheel for fiftyone-db (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fiftyone-db: filename=fiftyone_db-1.1-py3-none-manylinux1_x86_64.whl size=42156133 sha256=0877ee31aa1e33738bb6370aaf4a02c41cdad750c9e0982f6ae7abcc5a1e7641\n",
            "  Stored in directory: /root/.cache/pip/wheels/78/a6/41/ebc5ba9bd1c53e89d9c6684d72b1105166669f193326b99a73\n",
            "Successfully built fiftyone-db\n",
            "Installing collected packages: texttable, sseclient-py, pprintpp, kaleido, brotli, xmltodict, taskgroup, retrying, rarfile, pyzstd, pyppmd, pycryptodomex, pybcj, priority, multivolumefile, jsonlines, jmespath, inflate64, hyperframe, hpack, h11, graphql-core, ftfy, fiftyone-db, dnspython, dill, Deprecated, dacite, argcomplete, aiofiles, wsproto, strawberry-graphql, starlette, pymongo, py7zr, httpcore, h2, botocore, voxel51-eta, sse-starlette, s3transfer, motor, mongoengine, hypercorn, httpx, fiftyone-brain, universal-analytics-python3, boto3, fiftyone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Deprecated-1.2.14 aiofiles-23.2.1 argcomplete-3.2.1 boto3-1.34.7 botocore-1.34.7 brotli-1.1.0 dacite-1.7.0 dill-0.3.7 dnspython-2.4.2 fiftyone-0.23.2 fiftyone-brain-0.14.2 fiftyone-db-1.1 ftfy-6.1.3 graphql-core-3.2.3 h11-0.14.0 h2-4.1.0 hpack-4.0.0 httpcore-1.0.2 httpx-0.26.0 hypercorn-0.15.0 hyperframe-6.0.1 inflate64-1.0.0 jmespath-1.0.1 jsonlines-4.0.0 kaleido-0.2.1 mongoengine-0.24.2 motor-3.3.2 multivolumefile-0.2.3 pprintpp-0.4.0 priority-2.0.0 py7zr-0.20.8 pybcj-1.0.2 pycryptodomex-3.19.0 pymongo-4.6.1 pyppmd-1.1.0 pyzstd-0.15.9 rarfile-4.1 retrying-1.3.4 s3transfer-0.10.0 sse-starlette-0.10.3 sseclient-py-1.8.0 starlette-0.34.0 strawberry-graphql-0.138.1 taskgroup-0.0.0a4 texttable-1.7.0 universal-analytics-python3-1.1.1 voxel51-eta-0.12.1 wsproto-1.2.0 xmltodict-0.13.0\n"
          ]
        }
      ],
      "source": [
        "!pip install fiftyone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqv3lnaBLAdH"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "# Define a example CNN network\n",
        "class CNN_Model(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=1, class_num=5):\n",
        "        super(CNN_Model, self).__init__()\n",
        "        self.path = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=16, stride=16)\n",
        "        )\n",
        "        self.fc = nn.Linear(196, class_num)\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv_out = self.path(x)\n",
        "        flt = conv_out.view(conv_out.size(0), -1)\n",
        "        fc_out = self.fc(flt)\n",
        "        return fc_out\n",
        "\n",
        "# Create my own custom dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, ds, classes, trnsforms):\n",
        "        self.dataset = ds\n",
        "        self.classes = classes\n",
        "        self.transformer = trnsforms\n",
        "    # The main function\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.dataset[idx]\n",
        "        image = Image.open(sample.filepath).convert('RGB')\n",
        "\n",
        "        label = np.zeros(len(self.classes), dtype=np.float32)\n",
        "\n",
        "        for detection in sample.ground_truth.detections:\n",
        "            if detection.label in self.classes:\n",
        "                label[self.classes.index(detection.label)] = 1.0\n",
        "        image = self.transformer(image)\n",
        "\n",
        "        label = torch.tensor(label, dtype=torch.float64)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfSAY9GTVGPZ",
        "outputId": "31457fd5-ee22-4ef1-9533-b1fdeca8e334"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Migrating database to v0.23.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.migrations.runner:Migrating database to v0.23.2\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading split 'train' to '/root/fiftyone/coco-2017/train' if necessary\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.zoo.datasets:Downloading split 'train' to '/root/fiftyone/coco-2017/train' if necessary\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading annotations to '/root/fiftyone/coco-2017/tmp-download/annotations_trainval2017.zip'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.utils.coco:Downloading annotations to '/root/fiftyone/coco-2017/tmp-download/annotations_trainval2017.zip'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 100% |██████|    1.9Gb/1.9Gb [5.5s elapsed, 0s remaining, 377.2Mb/s]       \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:eta.core.utils: 100% |██████|    1.9Gb/1.9Gb [5.5s elapsed, 0s remaining, 377.2Mb/s]       \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting annotations to '/root/fiftyone/coco-2017/raw/instances_train2017.json'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.utils.coco:Extracting annotations to '/root/fiftyone/coco-2017/raw/instances_train2017.json'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading 10000 images\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.utils.coco:Downloading 10000 images\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 100% |██████████████| 10000/10000 [32.7m elapsed, 0s remaining, 5.0 images/s]      \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:eta.core.utils: 100% |██████████████| 10000/10000 [32.7m elapsed, 0s remaining, 5.0 images/s]      \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing annotations for 10000 downloaded samples to '/root/fiftyone/coco-2017/train/labels.json'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.utils.coco:Writing annotations for 10000 downloaded samples to '/root/fiftyone/coco-2017/train/labels.json'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset info written to '/root/fiftyone/coco-2017/info.json'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.zoo.datasets:Dataset info written to '/root/fiftyone/coco-2017/info.json'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading 'coco-2017' split 'train'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.zoo.datasets:Loading 'coco-2017' split 'train'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 100% |█████████████| 10000/10000 [2.7m elapsed, 0s remaining, 63.4 samples/s]      \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:eta.core.utils: 100% |█████████████| 10000/10000 [2.7m elapsed, 0s remaining, 63.4 samples/s]      \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset 'coco-2017-train-10000' created\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.zoo.datasets:Dataset 'coco-2017-train-10000' created\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading split 'validation' to '/root/fiftyone/coco-2017/validation' if necessary\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.zoo.datasets:Downloading split 'validation' to '/root/fiftyone/coco-2017/validation' if necessary\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found annotations at '/root/fiftyone/coco-2017/raw/instances_val2017.json'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.utils.coco:Found annotations at '/root/fiftyone/coco-2017/raw/instances_val2017.json'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading 1000 images\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.utils.coco:Downloading 1000 images\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 100% |████████████████| 1000/1000 [3.3m elapsed, 0s remaining, 4.6 images/s]      \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:eta.core.utils: 100% |████████████████| 1000/1000 [3.3m elapsed, 0s remaining, 4.6 images/s]      \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing annotations for 1000 downloaded samples to '/root/fiftyone/coco-2017/validation/labels.json'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.utils.coco:Writing annotations for 1000 downloaded samples to '/root/fiftyone/coco-2017/validation/labels.json'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset info written to '/root/fiftyone/coco-2017/info.json'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.zoo.datasets:Dataset info written to '/root/fiftyone/coco-2017/info.json'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading 'coco-2017' split 'validation'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.zoo.datasets:Loading 'coco-2017' split 'validation'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 100% |███████████████| 1000/1000 [17.4s elapsed, 0s remaining, 61.9 samples/s]      \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:eta.core.utils: 100% |███████████████| 1000/1000 [17.4s elapsed, 0s remaining, 61.9 samples/s]      \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset 'coco-2017-validation-1000' created\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.zoo.datasets:Dataset 'coco-2017-validation-1000' created\n"
          ]
        }
      ],
      "source": [
        "import fiftyone as fo\n",
        "import fiftyone.zoo as foz\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "# Selected classes\n",
        "classes = [\"person\", \"car\", \"dog\", \"cat\", \"bird\"]\n",
        "\n",
        "# Load the training dataset from the COCO zoo dataset\n",
        "dataset_train = foz.load_zoo_dataset(\n",
        "    \"coco-2017\",\n",
        "    split=\"train\",\n",
        "    label_types=[\"segmentations\"],\n",
        "    classes=classes,\n",
        "    max_samples=10000,\n",
        ")\n",
        "dataset_train = list(dataset_train)\n",
        "\n",
        "# Load the validation dataset from the COCO zoo dataset\n",
        "dataset_test = foz.load_zoo_dataset(\n",
        "    \"coco-2017\",\n",
        "    split=\"validation\",\n",
        "    label_types=[\"segmentations\"],\n",
        "    classes=classes,\n",
        "    max_samples=1000,\n",
        ")\n",
        "dataset_test = list(dataset_test)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gAUzzZ3um3wv"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "trainset = CustomDataset(ds=dataset_train, classes=classes, trnsforms=transforms)\n",
        "trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
        "\n",
        "testset = CustomDataset(ds=dataset_test, classes=classes, trnsforms=transforms)\n",
        "testloader = DataLoader(testset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lftrh4b8XynS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "070777d9-6f53-4116-c493-97a1064ac994"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/6 - Training: 100%|██████████| 313/313 [01:22<00:00,  3.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.3272 Total Acc: 0.9010\n",
            "Per Class Acc: [0.8458466529846191, 0.8292731642723083, 0.9356030225753784, 0.9355031847953796, 0.9589656591415405]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/6 - Training: 100%|██████████| 313/313 [01:15<00:00,  4.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2967 Total Acc: 0.9061\n",
            "Per Class Acc: [0.84944087266922, 0.8335663080215454, 0.9423921704292297, 0.9458865523338318, 0.9590654969215393]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/6 - Training: 100%|██████████| 313/313 [01:16<00:00,  4.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2912 Total Acc: 0.9060\n",
            "Per Class Acc: [0.8492411971092224, 0.8332667350769043, 0.9425918459892273, 0.9458865523338318, 0.9590654969215393]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/6 - Training: 100%|██████████| 313/313 [01:19<00:00,  3.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2886 Total Acc: 0.9057\n",
            "Per Class Acc: [0.8491413593292236, 0.8318690061569214, 0.9425918459892273, 0.945786714553833, 0.9590654969215393]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/6 - Training: 100%|██████████| 313/313 [01:15<00:00,  4.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2869 Total Acc: 0.9055\n",
            "Per Class Acc: [0.8489416837692261, 0.831569492816925, 0.942292332649231, 0.945786714553833, 0.9588658213615417]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/6 - Training: 100%|██████████| 313/313 [01:14<00:00,  4.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2859 Total Acc: 0.9056\n",
            "Per Class Acc: [0.8493410348892212, 0.831569492816925, 0.9423921704292297, 0.945786714553833, 0.9590654969215393]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 32/32 [00:07<00:00,  4.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Acc: 0.9029\n",
            "Per Class Acc: [0.8427734375, 0.83203125, 0.947265625, 0.93359375, 0.958984375]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Move the data to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Create the model\n",
        "model = CNN_Model(class_num=len(classes)).to(device)\n",
        "# Define optimizer and loss\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = Adam(model.parameters(), lr=0.001)\n",
        "# Update learning rate\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "\n",
        "epochs = 6\n",
        "\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    overall_accuracy = 0\n",
        "    accuracy_per_label = torch.zeros(len(classes), device=device)\n",
        "\n",
        "    # tqdm added for progress visualization\n",
        "    for i, data in tqdm(enumerate(trainloader, 0), total=len(trainloader), desc=f'Epoch {epoch + 1}/{epochs} - Training'):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        preds = torch.sigmoid(outputs) > 0.5\n",
        "        correct_predictions = (preds == labels).float()\n",
        "\n",
        "        accuracy_per_label += correct_predictions.sum(0) / len(labels)\n",
        "        overall_accuracy += correct_predictions.sum() / (len(labels) * len(classes))\n",
        "\n",
        "    accuracy_per_label /= len(trainloader)\n",
        "    running_loss /= len(trainloader)\n",
        "    overall_accuracy /= len(trainloader)\n",
        "\n",
        "    print(f'Train Loss: {running_loss:.4f} Total Acc: {overall_accuracy:.4f}')\n",
        "    print('Per Class Acc:', accuracy_per_label.tolist())\n",
        "\n",
        "# Testing loop\n",
        "with torch.no_grad():\n",
        "    overall_accuracy = 0\n",
        "    accuracy_per_label = torch.zeros(len(classes), device=device)\n",
        "\n",
        "    # tqdm added for progress visualization\n",
        "    for data in tqdm(testloader, total=len(testloader), desc='Testing'):\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        outputs = model(images)\n",
        "        preds = torch.sigmoid(outputs) > 0.5\n",
        "        correct_predictions = (preds == labels).float()\n",
        "\n",
        "        accuracy_per_label += correct_predictions.sum(0) / len(labels)\n",
        "        overall_accuracy += correct_predictions.sum() / (len(labels) * len(classes))\n",
        "\n",
        "    accuracy_per_label /= len(testloader)\n",
        "    overall_accuracy /= len(testloader)\n",
        "\n",
        "    print(f'Total Acc: {overall_accuracy:.4f}')\n",
        "    print('Per Class Acc:', accuracy_per_label.tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvO487Tmx_my"
      },
      "outputs": [],
      "source": [
        "import torchvision.ops\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class DeformableConv2d(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 kernel_size=3,\n",
        "                 stride=1,\n",
        "                 padding=1,\n",
        "                 bias=False):\n",
        "\n",
        "        super(DeformableConv2d, self).__init__()\n",
        "\n",
        "        self.padding = padding\n",
        "\n",
        "        self.offset_conv = nn.Conv2d(in_channels,\n",
        "                                     2 * kernel_size * kernel_size,\n",
        "                                     kernel_size=kernel_size,\n",
        "                                     stride=stride,\n",
        "                                     padding=self.padding,\n",
        "                                     bias=True)\n",
        "\n",
        "        nn.init.constant_(self.offset_conv.weight, 0.)\n",
        "        nn.init.constant_(self.offset_conv.bias, 0.)\n",
        "\n",
        "        self.modulator_conv = nn.Conv2d(in_channels,\n",
        "                                     1 * kernel_size * kernel_size,\n",
        "                                     kernel_size=kernel_size,\n",
        "                                     stride=stride,\n",
        "                                     padding=self.padding,\n",
        "                                     bias=True)\n",
        "\n",
        "        nn.init.constant_(self.modulator_conv.weight, 0.)\n",
        "        nn.init.constant_(self.modulator_conv.bias, 0.)\n",
        "\n",
        "        self.regular_conv = nn.Conv2d(in_channels=in_channels,\n",
        "                                      out_channels=out_channels,\n",
        "                                      kernel_size=kernel_size,\n",
        "                                      stride=stride,\n",
        "                                      padding=self.padding,\n",
        "                                      bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h, w = x.shape[2:]\n",
        "        max_offset = max(h, w)/4.\n",
        "\n",
        "        offset = self.offset_conv(x).clamp(-max_offset, max_offset)\n",
        "        modulator = 2. * torch.sigmoid(self.modulator_conv(x))\n",
        "\n",
        "        x = torchvision.ops.deform_conv2d(input=x,\n",
        "                                          offset=offset,\n",
        "                                          weight=self.regular_conv.weight,\n",
        "                                          bias=self.regular_conv.bias,\n",
        "                                          padding=self.padding,\n",
        "                                          mask=modulator\n",
        "                                          )\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuqTakxR2MvE"
      },
      "outputs": [],
      "source": [
        "# Define a example CNN network Deformable version\n",
        "class CNN_Model_DV(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=1, class_num=5):\n",
        "        super(CNN_Model_DV, self).__init__()\n",
        "        self.path = nn.Sequential(\n",
        "            # nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "            DeformableConv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=16, stride=16)\n",
        "        )\n",
        "        self.fc = nn.Linear(196, class_num)\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv_out = self.path(x)\n",
        "        flt = conv_out.view(conv_out.size(0), -1)\n",
        "        fc_out = self.fc(flt)\n",
        "        return fc_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQBOKzyg0jV0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cba4347e-f6b9-4664-e3b7-c9d850d06291"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/6: 100%|██████████| 313/313 [01:51<00:00,  2.80it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.3252 Total Acc: 0.9004\n",
            "Per Class Acc: [0.8395566940307617, 0.8245806694030762, 0.9405950307846069, 0.9449880123138428, 0.952276349067688]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/6: 100%|██████████| 313/313 [01:40<00:00,  3.12it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.2951 Total Acc: 0.9062\n",
            "Per Class Acc: [0.8495407104492188, 0.8338658213615417, 0.9425918459892273, 0.945786714553833, 0.9590654969215393]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/6: 100%|██████████| 313/313 [01:37<00:00,  3.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2894 Total Acc: 0.9059\n",
            "Per Class Acc: [0.8493410348892212, 0.8328673839569092, 0.9425918459892273, 0.945786714553833, 0.9589656591415405]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/6: 100%|██████████| 313/313 [01:38<00:00,  3.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2851 Total Acc: 0.9058\n",
            "Per Class Acc: [0.8493410348892212, 0.8322683572769165, 0.9424920082092285, 0.9458865523338318, 0.9590654969215393]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/6: 100%|██████████| 313/313 [01:38<00:00,  3.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2847 Total Acc: 0.9057\n",
            "Per Class Acc: [0.84944087266922, 0.8322683572769165, 0.9423921704292297, 0.9456868767738342, 0.9588658213615417]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/6: 100%|██████████| 313/313 [01:40<00:00,  3.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2843 Total Acc: 0.9057\n",
            "Per Class Acc: [0.8492411971092224, 0.8318690061569214, 0.9423921704292297, 0.9458865523338318, 0.9589656591415405]\n",
            "Total Acc: 0.9021\n",
            "Per Class Acc: [0.8427734375, 0.828125, 0.947265625, 0.93359375, 0.958984375]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "model = CNN_Model_DV(class_num=len(classes)).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = Adam(model.parameters(), lr=0.001)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "\n",
        "epochs = 6\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    overall_accuracy = 0\n",
        "    accuracy_per_label = torch.zeros(len(classes), device=device)\n",
        "\n",
        "    # Wrap your trainloader with tqdm for the progress bar\n",
        "    for i, data in enumerate(tqdm(trainloader, desc=f'Epoch {epoch+1}/{epochs}')):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        preds = torch.sigmoid(outputs) > 0.5\n",
        "        correct_predictions = (preds == labels).float()\n",
        "        accuracy_per_label += correct_predictions.sum(0) / len(labels)\n",
        "        overall_accuracy += correct_predictions.sum() / (len(labels) * len(classes))\n",
        "\n",
        "    accuracy_per_label /= len(trainloader)\n",
        "    running_loss /= len(trainloader)\n",
        "    overall_accuracy /= len(trainloader)\n",
        "\n",
        "    print(f'Train Loss: {running_loss:.4f} Total Acc: {overall_accuracy:.4f}')\n",
        "    print('Per Class Acc:', accuracy_per_label.tolist())\n",
        "\n",
        "    # Scheduler step at the end of each epoch\n",
        "    scheduler.step()\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    overall_accuracy = 0\n",
        "    accuracy_per_label = torch.zeros(len(classes), device=device)\n",
        "\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        outputs = model(images)\n",
        "        preds = torch.sigmoid(outputs) > 0.5\n",
        "        correct_predictions = (preds == labels).float()\n",
        "\n",
        "        accuracy_per_label += correct_predictions.sum(0)/(len(labels))\n",
        "\n",
        "        overall_accuracy += correct_predictions.sum()/(len(labels)*(len(classes)))\n",
        "\n",
        "    accuracy_per_label /= len(testloader)\n",
        "\n",
        "    overall_accuracy /= len(testloader)\n",
        "\n",
        "    print(f'Total Acc: {overall_accuracy:.4f}')\n",
        "    print('Per Class Acc:', accuracy_per_label.tolist())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The result has not been changed significantly; it is not relevant to epochs number because loss is stopped. We should enhance model complexity; It is going to show us the effect of deformable convolution significantly. Here, we could not do that because the deadline is just in few hours ."
      ],
      "metadata": {
        "id": "KzjuFK8aN_qb"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}